# Fast PyTranscriptor üé§

Utilidad de transcripci√≥n de audio multi-idioma optimizada para GPU con procesamiento en paralelo.

## Caracter√≠sticas ‚ú®

- **Multi-idioma**: Soporte para 100+ idiomas (especializado en espa√±ol/ingl√©s)
- **Procesamiento en paralelo**: Procesa m√∫ltiples archivos simult√°neamente
- **Optimizado para GPU**: Aprovecha tu GPU con Flash Attention 2
- **Flexible**: Acepta archivos individuales o directorios completos
- **M√∫ltiples formatos**: Salida en texto plano o subt√≠tulos SRT
- **Traducci√≥n**: Puede traducir autom√°ticamente al ingl√©s

## Requisitos üìã

- **Python 3.10** (requerido para Flash Attention 2)
- **NVIDIA GPU** con CUDA 11.8+ (probado en RTX 3060 con CUDA 13.0)
- **12GB+ VRAM** recomendado para mejor rendimiento

## Instalaci√≥n üîß

### Opci√≥n 1: Script autom√°tico (Recomendado)
```bash
chmod +x instalar.sh
./instalar.sh
```

### Opci√≥n 2: Manual con uv
```bash
# 1. Crear entorno virtual con Python 3.10
uv venv .venv --python 3.10
source .venv/bin/activate

# 2. Instalar PyTorch (ajusta seg√∫n tu versi√≥n de CUDA)
# Para CUDA 13.0:
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# Para CUDA 12.x:
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. Instalar dependencias base
uv pip install transformers accelerate optimum tqdm

# 4. Instalar Flash Attention 2 (opcional pero recomendado)
# Para CUDA 13.0 y Python 3.10:
uv pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.8.3+cu130torch2.9-cp310-cp310-linux_x86_64.whl
```

### Instalaci√≥n especial para CUDA 13.0

Si tienes CUDA 13.0 como yo, estos son los pasos exactos que funcionaron:

```bash
# 1. Aseg√∫rate de usar Python 3.10 (no 3.11)
python --version  # Debe mostrar 3.10.x

# 2. Instalar PyTorch para CUDA 13.0
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# 3. Instalar Flash Attention pre-compilado
uv pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.8.3+cu130torch2.9-cp310-cp310-linux_x86_64.whl

# 4. Instalar el resto de dependencias
uv pip install transformers accelerate optimum tqdm
```

## Uso üöÄ

### Ejemplos b√°sicos

**Transcribir un archivo:**
```bash
uv run python main.py audio.mp3 -o ./salida
```

**Transcribir un directorio completo:**
```bash
uv run python main.py ./videos -o ./transcripciones --workers 4
```

**Especificar idioma y traducir:**
```bash
uv run python main.py video.mp4 -o ./salida --idioma es --traducir
```

**Generar subt√≠tulos SRT:**
```bash
uv run python main.py video.mp4 -o ./salida --formato srt
```

### Opciones disponibles

| Opci√≥n | Descripci√≥n | Default |
|--------|-------------|---------|
| `entrada` | Archivo o directorio a procesar | Requerido |
| `-o, --salida` | Directorio donde guardar resultados | `./transcripciones` |
| `--idioma` | C√≥digo de idioma (es, en, fr, etc.) | Auto-detecta |
| `--traducir` | Traducir al ingl√©s | No |
| `--formato` | Formato de salida (txt, srt) | `txt` |
| `--workers` | N√∫mero de archivos a procesar en paralelo | `2` |
| `--gpu` | ID de GPU a usar | `0` |
| `--no-flash-attention` | Desactivar Flash Attention 2 | No |

## Formatos soportados üéµ

- Audio: `.mp3`, `.wav`, `.m4a`, `.flac`, `.ogg`, `.opus`
- Video: `.mp4`, `.avi`, `.mkv`, `.webm`

## Optimizaci√≥n GPU üéÆ

El script detecta autom√°ticamente tu GPU y aplica las siguientes optimizaciones:

1. **Flash Attention 2**: Reduce uso de memoria y aumenta velocidad
2. **Mixed Precision (FP16)**: Procesamiento m√°s r√°pido
3. **Torch Compile**: Optimizaci√≥n adicional del modelo
4. **Batch Processing**: Procesa m√∫ltiples segmentos simult√°neamente

### Verificar configuraci√≥n
```bash
# Verificar GPU
nvidia-smi

# Verificar instalaci√≥n
uv run python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"
uv run python -c "import torch; print(f'CUDA: {torch.version.cuda}')"
uv run python -c "import flash_attn; print(f'Flash Attention: {flash_attn.__version__}')"
```

## Rendimiento esperado ‚ö°

Con RTX 3060 (12GB VRAM):
- **Velocidad**: ~5-10x tiempo real (1 hora de audio en 6-12 minutos)
- **Memoria**: ~4-6 GB VRAM en uso
- **Paralelo**: Hasta 4 archivos simult√°neos recomendado

## Idiomas soportados üåç

Algunos de los idiomas m√°s comunes:
- Espa√±ol (es)
- Ingl√©s (en)
- Franc√©s (fr)
- Alem√°n (de)
- Italiano (it)
- Portugu√©s (pt)
- Ruso (ru)
- Chino (zh)
- Japon√©s (ja)
- Y 90+ m√°s...

## Tips de uso üí°

1. **Para videos de YouTube largos**: Usa `--workers 1` para evitar saturar la VRAM
2. **Para muchos archivos cortos**: Aumenta `--workers` hasta 4
3. **Si falla Flash Attention**: Usa `--no-flash-attention` (ser√° un poco m√°s lento)
4. **Para mejor calidad**: Especifica el idioma con `--idioma es` en vez de auto-detectar
5. **Monitorea tu GPU**: Usa `watch -n 1 nvidia-smi` en otra terminal

## Soluci√≥n de problemas üîß

### "CUDA out of memory"
- Reduce `--workers` a 1
- Usa `--no-flash-attention`
- Cierra otras aplicaciones que usen GPU

### "Flash Attention no se pudo instalar"
- Verifica que uses Python 3.10 (no 3.11+)
- El script funciona sin Flash Attention, solo ser√° m√°s lento
- Para CUDA 13.0, usa el wheel de mjun0812

### "No detecta mi GPU"
```bash
# Verifica drivers NVIDIA
nvidia-smi

# Verifica versi√≥n de CUDA
nvidia-smi | grep "CUDA Version"

# Reinstala PyTorch con el √≠ndice correcto
uv pip uninstall torch torchvision torchaudio
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130  # o cu121, cu118
```

### "uv es muy estricto con dependencias"
- Limpia `pyproject.toml` y `requirements.txt` de dependencias conflictivas
- Instala PyTorch y Flash Attention manualmente antes que otras dependencias
- Usa `--no-deps` cuando sea necesario

## Estructura de salida üìÅ

```
salida/
‚îú‚îÄ‚îÄ video1_transcripcion.txt
‚îú‚îÄ‚îÄ video2_transcripcion.txt
‚îî‚îÄ‚îÄ video3.srt (si usaste --formato srt)
```

## Ejemplo de archivo de salida (txt)
```
Archivo: /path/to/audio.mp3
Idioma: es
Traducido: No
--------------------------------------------------
[Aqu√≠ aparece la transcripci√≥n completa del audio]
```

## Roadmap futuro üó∫Ô∏è

Siguiendo la filosof√≠a "Hecho es Mejor que Perfecto", estas son mejoras planeadas:

- [ ] API REST para integraci√≥n con otros proyectos
- [ ] Soporte para diarizaci√≥n (identificar speakers)
- [ ] Interfaz web simple
- [ ] Exportar a m√°s formatos (VTT, JSON, WebVTT)
- [ ] Integraci√≥n con editores de video
- [ ] Procesamiento por lotes con archivo de configuraci√≥n
- [ ] Detecci√≥n autom√°tica de idioma mejorada
- [ ] Post-procesamiento de texto (puntuaci√≥n, formato)

## Contribuir ü§ù

¬øEncontraste un bug o tienes una idea? ¬°Abre un issue o PR!

## Licencia üìÑ

MIT - √ösalo como quieras para tus proyectos

---

#### **Filosof√≠a del proyecto** 
*"Hecho es Mejor que Perfecto" - Este es un MVP funcional. ¬°Empieza a usarlo y ve agregando features seg√∫n las necesites!*

#### **Agradecimientos**
- OpenAI por el modelo Whisper
- mjun0812 por los wheels pre-compilados de Flash Attention
- La comunidad de Hugging Face
